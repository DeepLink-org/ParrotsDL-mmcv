#include <float.h>
#include "common_mlu_helper.hpp"
#include <bang.h>
#include <sys/time.h>
#include <iostream>
__nram__ char nram_buffer[MAX_NRAM_SIZE];

namespace forward {

template <typename T>
__mlu_func__ void dmcn_im2col_bilinear(const T *input, const int data_width,
                                  const int height, const int width, T* h, T* w,
                                  T* val, T* cond, const int width_col, const uint32_t width_stride) {
    // __bang_gt_scalar(cond, h_im, -1, width_stride);
    T* h_low = (T*)(cond + width_stride);
    T* w_low = h_low + width_stride;
    T* h_high = w_low + width_stride;
    T* w_high = h_high + width_stride;
    T* lh = (T*)(w_high + width_stride);
    T* lw = lh + width_stride;
    T* hh = lw + width_stride;
    T* hw = hh + width_stride;
    T* v1 = hw + width_stride;
    T* v2 = v1 + width_stride;
    T* v3 = v2 + width_stride;
    T* v4 = v3 + width_stride;
    T* w1 = v4 + width_stride;
    T* w2 = w1 + width_stride;
    T* w3 = w2 + width_stride;
    T* w4 = w3 + width_stride;
    T* v1_temp = w4 + width_stride;
    T* v2_temp = v1_temp + width_stride;
    T* v3_temp = v2_temp + width_stride;
    T* v4_temp = v3_temp + width_stride;


    for (uint32_t w_col = 0; w_col < width_col; w_col++) {
        h_low[w_col] = floorf(h[w_col]);
        w_low[w_col] = floorf(w[w_col]);
    }

    __bang_add_const(h_high, h_low, 1, width_stride);
    __bang_add_const(w_high, w_low, 1, width_stride);
    __bang_sub(lh, h, h_low, width_stride);
    __bang_sub(lw, w, w_low, width_stride);
    __bang_sub(hh, h_high, h, width_stride);
    __bang_sub(hw, w_high, w, width_stride);

    __bang_mul_const(v1_temp, h_low, data_width, width_stride);
    __bang_add(v2_temp, v1_temp, w_high, width_stride);
    __bang_add(v1_temp, v1_temp, w_low, width_stride);

    __bang_mul_const(v3_temp, h_high, data_width, width_stride);
    __bang_add(v4_temp, v3_temp, w_high, width_stride);
    __bang_add(v3_temp, v3_temp, w_low, width_stride);


    T* h_cond1 = v4_temp + width_stride;
    T* h_cond2 = h_cond1 + width_stride;
    T* w_cond1 = h_cond2 + width_stride;
    T* w_cond2 = w_cond1 + width_stride;
    T* h_cond3 = w_cond2 + width_stride;
    T* h_cond4 = h_cond3 + width_stride;
    T* w_cond3 = h_cond4 + width_stride;
    T* w_cond4 = w_cond3 + width_stride;

    __nramset(h_cond1, width_stride, (T)-1);
    __bang_gt(h_cond1, h, h_cond1, width_stride);
    __nramset(h_cond2, width_stride, (T)height);
    __bang_lt(h_cond2, h, h_cond2, width_stride);
    __bang_and(h_cond1, h_cond1, h_cond2, width_stride);


    __nramset(w_cond1, width_stride, (T)-1);
    __bang_gt(w_cond1, w, w_cond1, width_stride);
    __nramset(w_cond2, width_stride, (T)width);
    __bang_lt(w_cond2, w, w_cond2, width_stride);
    __bang_and(w_cond1, w_cond1, w_cond2, width_stride);
    __bang_and(cond, h_cond1, w_cond1, width_stride);

    __nramset(h_cond3, width_stride, (T)0);
    __bang_ge(h_cond3, h, h_cond3, width_stride);
    __nramset(h_cond4, width_stride, (T)(height-1));
    __bang_le(h_cond4, h, h_cond4, width_stride);
    
    __nramset(w_cond3, width_stride, (T)0);
    __bang_ge(w_cond3, w, w_cond3, width_stride);
    __nramset(w_cond4, width_stride, (T)(width-1));
    __bang_le(w_cond4, w, w_cond4, width_stride);

    __bang_and(h_cond1, cond, h_cond3, width_stride);
    __bang_and(h_cond1, h_cond1, w_cond3, width_stride);
    __bang_and(h_cond2, cond, h_cond3, width_stride);
    __bang_and(h_cond2, h_cond2, w_cond4, width_stride);
    __bang_and(w_cond1, cond, h_cond4, width_stride);
    __bang_and(w_cond1, w_cond1, w_cond3, width_stride);
    __bang_and(w_cond2, cond, h_cond4, width_stride);
    __bang_and(w_cond2, w_cond2, w_cond4, width_stride);

    __bang_mul(v1_temp, v1_temp, h_cond1, width_stride);
    __bang_mul(v2_temp, v2_temp, h_cond2, width_stride);
    __bang_mul(v3_temp, v3_temp, w_cond1, width_stride);
    __bang_mul(v4_temp, v4_temp, w_cond2, width_stride);
    
    for (uint32_t w_col = 0; w_col < width_col; w_col++) {
        __memcpy_async(v1 + w_col, input + (int)v1_temp[w_col], sizeof(T), GDRAM2NRAM);
        __memcpy_async(v2 + w_col, input + (int)v2_temp[w_col], sizeof(T), GDRAM2NRAM);
        __memcpy_async(v3 + w_col, input + (int)v3_temp[w_col], sizeof(T), GDRAM2NRAM);
        __memcpy_async(v4 + w_col, input + (int)v4_temp[w_col], sizeof(T), GDRAM2NRAM);
    }
    __asm__ volatile("sync;");
    __bang_mul(v1, v1, h_cond1, width_stride);
    __bang_mul(v2, v2, h_cond2, width_stride);
    __bang_mul(v3, v3, w_cond1, width_stride);
    __bang_mul(v4, v4, w_cond2, width_stride);

    __bang_mul(w1, hh, hw, width_stride);
    __bang_mul(w2, hh, lw, width_stride);
    __bang_mul(w3, lh, hw, width_stride);
    __bang_mul(w4, lh, lw, width_stride);
    __bang_mul(w1, w1, v1, width_stride);
    __bang_mul(w2, w2, v2, width_stride);
    __bang_mul(w3, w3, v3, width_stride);
    __bang_mul(w4, w4, v4, width_stride);
    __bang_add(w1, w1, w2, width_stride);
    __bang_add(w3, w3, w4, width_stride);
    __bang_add(val, w1, w3, width_stride);


}



template <typename T>
__mlu_func__ void modulated_deformable_im2col_camb_block(
        const int n, const int core_nums, const T *data_im, const T *data_offset, const T *data_mask,
        const int height, const int width, const int kernel_h, const int kernel_w,
        const int pad_h, const int pad_w, const int stride_h, const int stride_w,
        const int dilation_h, const int dilation_w,
        const int channel_per_deformable_group, const int batch_size,
        const int num_channels, const int deformable_group, const int height_col,
        const int width_col, T *data_col) {
    uint32_t idx = taskId;
    if (idx >= n) return;
    // struct timeval begin, end;
    // gettimeofday(&begin, NULL);
    const uint32_t nfu_align_num = NFU_ALIGN_SIZE / sizeof(T);
    const uint32_t width_stride = ((width_col - 1) / nfu_align_num + 1) * nfu_align_num;
    for (uint32_t index = idx * width_col; index < n; index += core_nums * width_col) {
        // index index of output matrix
        // const int w_col = index % width_col;
        const int h_col = (index / width_col) % height_col;
        const int b_col = (index / width_col / height_col) % batch_size;
        const int c_im = (index / width_col / height_col) / batch_size;
        const int c_col = c_im * kernel_h * kernel_w;

        // compute deformable group index
        const int deformable_group_index = c_im / channel_per_deformable_group;

        const int h_in = h_col * stride_h - pad_h;
        // const int w_in = w_col * stride_w - pad_w;
        int* w_in = (int*)nram_buffer;

        for (uint32_t w_col = 0; w_col < width_col; w_col++) {
            w_in[w_col] = int(w_col * stride_w - pad_w);
            // if (taskId == 0) __bang_printf("w_in[w_col] = %d \n", w_in[w_col]);
        }

        T *data_col_ptr =
            data_col + ((c_col * batch_size + b_col) * height_col + h_col) * width_col;
        const T *data_im_ptr =
            data_im + (b_col * num_channels + c_im) * height * width;
        const T *data_offset_ptr =
            data_offset + (b_col * deformable_group + deformable_group_index) * 2 *
                            kernel_h * kernel_w * height_col * width_col;

        const T *data_mask_ptr =
            data_mask + (b_col * deformable_group + deformable_group_index) *
                            kernel_h * kernel_w * height_col * width_col;

        for (int i = 0; i < kernel_h; ++i) {
            for (int j = 0; j < kernel_w; ++j) {
                const int data_offset_h_ptr =
                    ((2 * (i * kernel_w + j)) * height_col + h_col) * width_col;
                const int data_offset_w_ptr =
                    ((2 * (i * kernel_w + j) + 1) * height_col + h_col) * width_col;
                const int data_mask_hw_ptr =
                    ((i * kernel_w + j) * height_col + h_col) * width_col;

                T* offset_h = (T*)(w_in + width_stride);
                T* offset_w = offset_h + width_stride;
                T* mask = offset_w + width_stride;
                T* h_im = mask + width_stride;
                T* w_im = h_im + width_stride;
                T* val = w_im + width_stride;
                T* cond = val + width_stride;

                __memcpy_async(offset_h, data_offset_ptr + data_offset_h_ptr, width_col * sizeof(T), GDRAM2NRAM);
                __memcpy_async(offset_w, data_offset_ptr + data_offset_w_ptr, width_col * sizeof(T), GDRAM2NRAM);
                __memcpy_async(mask, data_mask_ptr + data_mask_hw_ptr, width_col * sizeof(T), GDRAM2NRAM);
                __asm__ volatile("sync;");
                
                __bang_add_const(offset_h, offset_h, (T)(h_in + i * dilation_h), width_stride);
                h_im = offset_h;
                
                __bang_add_const(offset_w, offset_w, (T)(j * dilation_w), width_stride);

                for (uint32_t w_col = 0; w_col < width_col; w_col++) {
                    // h_im[w_col] = h_in + i * dilation_h + offset_h[w_col];
                    w_im[w_col] = w_in[w_col] + offset_w[w_col];
                }

                dmcn_im2col_bilinear(data_im_ptr, width, height, width, h_im, w_im, val, cond, width_col, width_stride);

                __bang_mul(val, val, mask, width_stride);
                __asm__ volatile("sync;");
                __memcpy_async(data_col_ptr, val, width_col * sizeof(T), NRAM2GDRAM);
                data_col_ptr += batch_size * height_col * width_col;
            }
        }
    }

    // gettimeofday(&end, NULL);
    // __bang_printf("Time consumed (device): %fus\n",
    //     (float)((end.tv_sec - begin.tv_sec) * 1000000
    //     + (end.tv_usec - begin.tv_usec)));
}


template <typename T>
__mlu_global__ void modulated_deformable_im2col_camb_kernel(
        const int n, const int core_nums, const void *data_im, const void *data_offset, const T *data_mask,
        const int height, const int width, const int kernel_h, const int kernel_w,
        const int pad_h, const int pad_w, const int stride_h, const int stride_w,
        const int dilation_h, const int dilation_w,
        const int channel_per_deformable_group, const int batch_size,
        const int num_channels, const int deformable_group, const int height_col,
        const int width_col, void *data_col) {
    modulated_deformable_im2col_camb_block(
        n, core_nums, (const T*)data_im, (const T*)data_offset, (const T*)data_mask,
        height, width, kernel_h, kernel_w,
        pad_h, pad_w, stride_h, stride_w,
        dilation_h, dilation_w,
        channel_per_deformable_group, batch_size,
        num_channels, deformable_group, height_col,
        width_col, (T*)data_col
    );
}

}  // namespace forward

void modulated_deformable_im2col_camb(
        cnrtDim3_t k_dim, cnrtFunctionType_t k_type, cnrtQueue_t queue, const cnrtDataType_t d_type,
        const void* data_im, const void* data_offset, const void* data_mask,
        const int batch_size, const int channels, const int height_im,
        const int width_im, const int height_col, const int width_col,
        const int kernel_h, const int kernel_w, const int pad_h, const int pad_w,
        const int stride_h, const int stride_w, const int dilation_h,
        const int dilation_w, const int deformable_group, void* data_col) {
    const int channel_per_deformable_group = channels / deformable_group;
    const int num_kernels = channels * batch_size * height_col * width_col;
    const int core_nums = k_dim.x * k_dim.y * k_dim.z;
    if (d_type == CNRT_FLOAT16) {
        forward::modulated_deformable_im2col_camb_kernel<
            half><<<k_dim, k_type, queue>>>(num_kernels, core_nums, (const half*)data_im, (const half*)data_offset, (const half*)data_mask,
                                            height_im, width_im, kernel_h, kernel_w,
                                            pad_h, pad_w, stride_h, stride_w,
                                            dilation_h, dilation_w,
                                            channel_per_deformable_group, batch_size,
                                            channels, deformable_group, height_col,
                                            width_col, (half*)data_col);
    } else {
        forward::modulated_deformable_im2col_camb_kernel<
            float><<<k_dim, k_type, queue>>>(num_kernels, core_nums, (const float*)data_im, (const float*)data_offset, (const float*)data_mask,
                                            height_im, width_im, kernel_h, kernel_w,
                                            pad_h, pad_w, stride_h, stride_w,
                                            dilation_h, dilation_w,
                                            channel_per_deformable_group, batch_size,
                                            channels, deformable_group, height_col,
                                            width_col, (float*)data_col);
    }
}